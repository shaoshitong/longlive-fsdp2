"""
DCP Checkpointè½¬æ¢å™¨
å°†PyTorch DCPæ ¼å¼çš„checkpointè½¬æ¢ä¸ºæ ‡å‡†torch.saveæ ¼å¼ï¼Œç”¨äºæ¨ç†
"""

import torch
import torch.distributed.checkpoint as dcp
from pathlib import Path
import logging
from typing import Dict, Any, Optional, Union
import shutil
import tempfile

logger = logging.getLogger(__name__)


class DCPCheckpointConverter:
    """
    DCP Checkpointè½¬æ¢å™¨
    
    å°†åˆ†å¸ƒå¼è®­ç»ƒä¿å­˜çš„DCPæ ¼å¼checkpointè½¬æ¢ä¸ºæ¨ç†å‹å¥½çš„torch.saveæ ¼å¼
    """
    
    def __init__(self, verbose: bool = True):
        """
        åˆå§‹åŒ–è½¬æ¢å™¨
        
        Args:
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
        """
        self.verbose = verbose
        if self.verbose:
            logging.basicConfig(level=logging.INFO)
    
    def convert(
        self, 
        dcp_checkpoint_path: Union[str, Path],
        output_path: Union[str, Path],
        include_optimizer: bool = False,
        include_scheduler: bool = False,
        model_only: bool = False
    ) -> bool:
        """
        è½¬æ¢DCP checkpointåˆ°torch.saveæ ¼å¼
        
        Args:
            dcp_checkpoint_path: DCP checkpointç›®å½•è·¯å¾„
            output_path: è¾“å‡ºçš„.pthæ–‡ä»¶è·¯å¾„
            include_optimizer: æ˜¯å¦åŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€
            include_scheduler: æ˜¯å¦åŒ…å«è°ƒåº¦å™¨çŠ¶æ€
            model_only: æ˜¯å¦åªä¿å­˜æ¨¡å‹æƒé‡ï¼ˆæ¨ç†æ¨èï¼‰
            
        Returns:
            bool: è½¬æ¢æ˜¯å¦æˆåŠŸ
        """
        try:
            dcp_path = Path(dcp_checkpoint_path)
            output_path = Path(output_path)
            
            if not dcp_path.exists():
                logger.error(f"DCP checkpointè·¯å¾„ä¸å­˜åœ¨: {dcp_path}")
                return False
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"å¼€å§‹è½¬æ¢DCP checkpoint...")
            logger.info(f"æºè·¯å¾„: {dcp_path}")
            logger.info(f"ç›®æ ‡è·¯å¾„: {output_path}")
            
            # åŠ è½½DCP checkpoint
            state_dict = self._load_dcp_checkpoint(dcp_path)
            if state_dict is None:
                return False
            
            # æ„å»ºè¾“å‡ºçŠ¶æ€å­—å…¸
            output_state = self._build_output_state(
                state_dict, 
                include_optimizer=include_optimizer,
                include_scheduler=include_scheduler,
                model_only=model_only
            )
            
            # ä¿å­˜ä¸ºtorch.saveæ ¼å¼
            self._save_torch_checkpoint(output_state, output_path)
            
            # éªŒè¯è½¬æ¢ç»“æœ
            if self._verify_converted_checkpoint(output_path):
                logger.info(f"âœ… DCP checkpointè½¬æ¢æˆåŠŸ!")
                self._print_checkpoint_info(output_state, output_path)
                return True
            else:
                logger.error("âŒ è½¬æ¢åçš„checkpointéªŒè¯å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"âŒ DCP checkpointè½¬æ¢å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False
    
    def _load_dcp_checkpoint(self, dcp_path: Path) -> Optional[Dict[str, Any]]:
        """åŠ è½½DCP checkpoint"""
        try:
            logger.info("ğŸ“¥ åŠ è½½DCP checkpoint...")
            
            # æ£€æŸ¥DCPæ–‡ä»¶
            distcp_files = list(dcp_path.glob("*.distcp"))
            if not distcp_files:
                logger.error(f"åœ¨ {dcp_path} ä¸­æœªæ‰¾åˆ°.distcpæ–‡ä»¶")
                return None
            
            logger.info(f"æ‰¾åˆ° {len(distcp_files)} ä¸ªDCPæ–‡ä»¶")
            
            # ä½¿ç”¨PyTorch DCPçš„format_utilsè¿›è¡Œè½¬æ¢
            # è¿™æ˜¯å®˜æ–¹æ¨èçš„DCPâ†’torch.saveè½¬æ¢æ–¹æ³•
            logger.info("ä½¿ç”¨format_utilsè½¬æ¢DCP checkpoint...")
            
            import tempfile
            from torch.distributed.checkpoint.format_utils import dcp_to_torch_save
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.pth', delete=False) as tmp_file:
                temp_path = tmp_file.name
            
            # è½¬æ¢DCPåˆ°torch.saveæ ¼å¼
            dcp_to_torch_save(str(dcp_path), temp_path)
            
            # åŠ è½½è½¬æ¢åçš„æ–‡ä»¶ - å¤„ç†OmegaConfé—®é¢˜
            try:
                # å…ˆå°è¯•å®‰å…¨åŠ è½½
                state_dict = torch.load(temp_path, map_location='cpu', weights_only=True)
            except Exception:
                logger.info("æ£€æµ‹åˆ°OmegaConfç±»å‹ï¼Œä½¿ç”¨å…¼å®¹æ¨¡å¼åŠ è½½...")
                # æ·»åŠ OmegaConfåˆ°å®‰å…¨å…¨å±€åˆ—è¡¨
                torch.serialization.add_safe_globals([
                    'omegaconf.listconfig.ListConfig', 
                    'omegaconf.dictconfig.DictConfig'
                ])
                state_dict = torch.load(temp_path, map_location='cpu', weights_only=False)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            import os
            os.unlink(temp_path)
            
            logger.info(f"âœ… DCP checkpointåŠ è½½æˆåŠŸ")
            logger.info(f"åŒ…å«çš„é”®: {list(state_dict.keys())}")
            
            # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            for key, value in state_dict.items():
                if isinstance(value, dict):
                    logger.info(f"  ğŸ”‘ {key}: {len(value)} ä¸ªå­é¡¹")
                else:
                    logger.info(f"  ğŸ”‘ {key}: {type(value).__name__}")
            
            return state_dict
            
        except Exception as e:
            logger.error(f"åŠ è½½DCP checkpointå¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return None
    
    def _build_output_state(
        self, 
        dcp_state: Dict[str, Any],
        include_optimizer: bool = False,
        include_scheduler: bool = False,
        model_only: bool = False
    ) -> Dict[str, Any]:
        """æ„å»ºè¾“å‡ºçŠ¶æ€å­—å…¸"""
        logger.info("ğŸ”§ æ„å»ºè¾“å‡ºçŠ¶æ€å­—å…¸...")
        
        output_state = {}
        
        # 1. æ¨¡å‹æƒé‡ï¼ˆå¿…éœ€ï¼‰
        if "model" in dcp_state:
            output_state["model"] = dcp_state["model"]
            logger.info(f"âœ… åŒ…å«æ¨¡å‹æƒé‡: {len(dcp_state['model'])} ä¸ªå‚æ•°")
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹æƒé‡")
        
        # 2. EMAæƒé‡ï¼ˆæ¨ç†é‡è¦ï¼‰
        if "ema" in dcp_state:
            output_state["ema"] = dcp_state["ema"]
            logger.info("âœ… åŒ…å«EMAæƒé‡")
        else:
            logger.info("â„¹ï¸ æœªæ‰¾åˆ°EMAæƒé‡")
        
        # å¦‚æœåªè¦æ¨¡å‹ï¼Œè·³è¿‡å…¶ä»–ç»„ä»¶
        if model_only:
            logger.info("ğŸ“¦ æ¨¡å‹ä¸“ç”¨æ¨¡å¼ï¼šåªä¿ç•™æ¨¡å‹å’ŒEMAæƒé‡")
            return output_state
        
        # 3. ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
        if include_optimizer and "optimizer" in dcp_state:
            output_state["optimizer"] = dcp_state["optimizer"]
            logger.info("âœ… åŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€")
        
        # 4. è°ƒåº¦å™¨çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
        if include_scheduler and "scheduler" in dcp_state:
            output_state["scheduler"] = dcp_state["scheduler"]
            logger.info("âœ… åŒ…å«è°ƒåº¦å™¨çŠ¶æ€")
        
        # 5. è®­ç»ƒå…ƒæ•°æ®
        metadata = {}
        for key, value in dcp_state.items():
            if key not in ["model", "optimizer", "ema", "scheduler"]:
                if isinstance(value, (int, float, str, bool)):
                    metadata[key] = value
        
        if metadata:
            output_state["metadata"] = metadata
            logger.info(f"âœ… åŒ…å«å…ƒæ•°æ®: {list(metadata.keys())}")
        
        return output_state
    
    def _save_torch_checkpoint(self, state_dict: Dict[str, Any], output_path: Path):
        """ä¿å­˜ä¸ºtorch.saveæ ¼å¼"""
        logger.info(f"ğŸ’¾ ä¿å­˜ä¸ºtorch.saveæ ¼å¼...")
        
        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¡®ä¿åŸå­æ€§å†™å…¥
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pth') as tmp_file:
            torch.save(state_dict, tmp_file.name)
            tmp_path = tmp_file.name
        
        # ç§»åŠ¨åˆ°æœ€ç»ˆä½ç½®
        shutil.move(tmp_path, output_path)
        logger.info(f"âœ… ä¿å­˜å®Œæˆ: {output_path}")
    
    def _verify_converted_checkpoint(self, checkpoint_path: Path) -> bool:
        """éªŒè¯è½¬æ¢åçš„checkpoint"""
        try:
            logger.info("ğŸ” éªŒè¯è½¬æ¢åçš„checkpoint...")
            
            # å°è¯•åŠ è½½ - å¤„ç†OmegaConfé—®é¢˜
            try:
                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
            except Exception:
                # å¦‚æœæœ‰OmegaConfï¼Œä½¿ç”¨å…¼å®¹æ¨¡å¼
                torch.serialization.add_safe_globals([
                    'omegaconf.listconfig.ListConfig', 
                    'omegaconf.dictconfig.DictConfig'
                ])
                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
            
            # æ£€æŸ¥åŸºæœ¬ç»“æ„
            if not isinstance(checkpoint, dict):
                logger.error("Checkpointä¸æ˜¯å­—å…¸æ ¼å¼")
                return False
            
            # æ£€æŸ¥æ¨¡å‹æƒé‡
            if "model" not in checkpoint:
                logger.error("ç¼ºå°‘æ¨¡å‹æƒé‡")
                return False
            
            model_params = checkpoint["model"]
            if not isinstance(model_params, dict) or len(model_params) == 0:
                logger.error("æ¨¡å‹æƒé‡ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")
                return False
            
            logger.info(f"âœ… CheckpointéªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"éªŒè¯checkpointå¤±è´¥: {e}")
            import traceback
            logger.error(f"éªŒè¯è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False
    
    def _print_checkpoint_info(self, state_dict: Dict[str, Any], output_path: Path):
        """æ‰“å°checkpointä¿¡æ¯"""
        logger.info("\n" + "="*50)
        logger.info("ğŸ“Š è½¬æ¢åçš„Checkpointä¿¡æ¯:")
        logger.info("="*50)
        
        # æ–‡ä»¶å¤§å°
        file_size_mb = output_path.stat().st_size / (1024 * 1024)
        logger.info(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")
        
        # ç»„ä»¶ä¿¡æ¯
        components = []
        if "model" in state_dict:
            model_params = len(state_dict["model"])
            components.append(f"æ¨¡å‹å‚æ•°: {model_params:,} ä¸ª")
        
        if "ema" in state_dict:
            components.append("EMAæƒé‡: âœ…")
        
        if "optimizer" in state_dict:
            components.append("ä¼˜åŒ–å™¨çŠ¶æ€: âœ…")
        
        if "metadata" in state_dict:
            metadata = state_dict["metadata"]
            if "step" in metadata:
                components.append(f"è®­ç»ƒæ­¥æ•°: {metadata['step']}")
        
        for component in components:
            logger.info(f"ğŸ“¦ {component}")
        
        logger.info("="*50)
        logger.info(f"âœ… å¯ç›´æ¥ç”¨äºæ¨ç†: torch.load('{output_path}')")
        logger.info("="*50 + "\n")


def convert_dcp_checkpoint(
    dcp_path: str,
    output_path: str,
    model_only: bool = True,
    include_optimizer: bool = False,
    include_scheduler: bool = False,
    verbose: bool = True
) -> bool:
    """
    ä¾¿æ·å‡½æ•°ï¼šè½¬æ¢DCP checkpoint
    
    Args:
        dcp_path: DCP checkpointç›®å½•è·¯å¾„
        output_path: è¾“å‡º.pthæ–‡ä»¶è·¯å¾„
        model_only: æ˜¯å¦åªä¿å­˜æ¨¡å‹æƒé‡ï¼ˆæ¨èæ¨ç†ä½¿ç”¨ï¼‰
        include_optimizer: æ˜¯å¦åŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€
        include_scheduler: æ˜¯å¦åŒ…å«è°ƒåº¦å™¨çŠ¶æ€
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
    
    Returns:
        bool: æ˜¯å¦è½¬æ¢æˆåŠŸ
    """
    converter = DCPCheckpointConverter(verbose=verbose)
    return converter.convert(
        dcp_checkpoint_path=dcp_path,
        output_path=output_path,
        include_optimizer=include_optimizer,
        include_scheduler=include_scheduler,
        model_only=model_only
    )


if __name__ == "__main__":
    # æµ‹è¯•è½¬æ¢
    import sys
    
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python dcp_checkpoint_converter.py <dcp_path> <output_path>")
        sys.exit(1)
    
    dcp_path = sys.argv[1]
    output_path = sys.argv[2]
    
    success = convert_dcp_checkpoint(dcp_path, output_path)
    sys.exit(0 if success else 1)
